import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, classification_report
from scipy.signal import savgol_filter
from sklearn.cross_decomposition import PLSRegression
from scipy.interpolate import interp1d
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import autokeras as ak
import tensorflow as tf
import warnings
warnings.filterwarnings('ignore')

# GPUé…ç½®
def configure_gpu():
    """é…ç½®GPUä½¿ç”¨"""
    physical_devices = tf.config.list_physical_devices('GPU')
    print(f"æ£€æµ‹åˆ° {len(physical_devices)} ä¸ªGPUè®¾å¤‡:")
    for device in physical_devices:
        print(f"  - {device}")
    
    if len(physical_devices) > 0:
        try:
            for device in physical_devices:
                tf.config.experimental.set_memory_growth(device, True)
            print("âœ… GPUé…ç½®æˆåŠŸï¼Œå¯ç”¨å†…å­˜åŠ¨æ€å¢é•¿")
            
            policy = tf.keras.mixed_precision.Policy('mixed_float16')
            tf.keras.mixed_precision.set_global_policy(policy)
            print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿ")
            
            return True
        except Exception as e:
            print(f"âš ï¸ GPUé…ç½®å¤±è´¥: {e}")
            return False
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False

class CARS:
    """CARSç‰¹å¾é€‰æ‹©ç®—æ³•"""
    def __init__(self, n_iterations=50, cv_folds=5):
        self.n_iterations = n_iterations
        self.cv_folds = cv_folds
        self.best_feature_indices_ = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.best_rmse_ = float('inf')
        
        retained_feature_indices_history = []
        retained_indices = list(range(n_features))

        for i in range(self.n_iterations):
            pls = PLSRegression(n_components=min(10, len(retained_indices), n_samples - 1))
            pls.fit(X[:, retained_indices], y)
            weights = np.abs(pls.coef_).flatten()
            
            ratio = (1 / (i + 1)) ** 0.3
            n_retained = max(2, int(n_features * ratio))
            
            sorted_indices = np.argsort(weights)[::-1]
            retained_indices_local = sorted_indices[:n_retained]
            retained_indices = np.array(retained_indices)[retained_indices_local]
            retained_feature_indices_history.append(retained_indices)

            if len(retained_indices) < 2:
                break

        print("æ­£åœ¨è¯„ä¼°ç‰¹å¾å­é›†...")
        for i, indices in enumerate(retained_feature_indices_history):
            if len(indices) < 2:
                continue
            X_subset = X[:, indices]
            kf = KFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
            rmse_cv_scores = []
            for train_idx, val_idx in kf.split(X_subset):
                X_train_cv, X_val_cv = X_subset[train_idx], X_subset[val_idx]
                y_train_cv, y_val_cv = y[train_idx], y[val_idx]
                
                pls_cv = PLSRegression(n_components=min(10, X_train_cv.shape[1], X_train_cv.shape[0] - 1))
                pls_cv.fit(X_train_cv, y_train_cv)
                y_pred_cv = pls_cv.predict(X_val_cv)
                rmse_cv_scores.append(np.sqrt(mean_squared_error(y_val_cv, y_pred_cv)))

            avg_rmse = np.mean(rmse_cv_scores)

            if avg_rmse < self.best_rmse_:
                self.best_rmse_ = avg_rmse
                self.best_feature_indices_ = indices
        
        if self.best_feature_indices_ is None and retained_feature_indices_history:
             self.best_feature_indices_ = retained_feature_indices_history[-1]

        return self

    def transform(self, X):
        return X[:, self.best_feature_indices_]

class BaselineDriftAugmentation:
    """åŸºçº¿æ¼‚ç§»æ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self, random_state=42):
        np.random.seed(random_state)
        self.random_state = random_state
    
    def add_baseline_drift(self, spectrum, drift_strength=0.05):
        """æ·»åŠ åŸºçº¿æ¼‚ç§»"""
        n_points = len(spectrum)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ§åˆ¶ç‚¹è¿›è¡Œæ’å€¼
        min_points = max(4, min(6, n_points // 10))
        max_points = min(10, n_points // 5)
        drift_points = np.random.randint(min_points, max_points + 1)
        
        if drift_points >= n_points:
            drift_points = n_points - 1
            
        # æ€»æ˜¯åŒ…å«é¦–å°¾ä¸¤ä¸ªç‚¹ä»¥ç¡®ä¿è¾¹ç•Œæ¡ä»¶
        control_indices = [0, n_points - 1]
        
        if drift_points > 2:
            middle_points = np.random.choice(
                range(1, n_points - 1), 
                size=drift_points - 2, 
                replace=False
            )
            control_indices.extend(middle_points)
        
        control_x = np.sort(control_indices)
        control_y = np.random.normal(0, drift_strength, len(control_x))
        
        # ç¡®ä¿è¾¹ç•Œç‚¹çš„æ¼‚ç§»è¾ƒå°
        control_y[0] *= 0.5
        control_y[-1] *= 0.5
        
        try:
            if len(control_x) >= 4:
                f = interp1d(control_x, control_y, kind='cubic', 
                            bounds_error=False, fill_value=0)
            else:
                f = interp1d(control_x, control_y, kind='linear', 
                            bounds_error=False, fill_value=0)
            
            baseline = f(np.arange(n_points))
            
            if np.any(np.isnan(baseline)) or np.any(np.isinf(baseline)):
                baseline = np.linspace(control_y[0], control_y[-1], n_points)
                
        except Exception as e:
            print(f"âš ï¸ æ’å€¼å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ¼‚ç§»: {e}")
            baseline = np.random.normal(0, drift_strength * 0.5, n_points)
        
        return spectrum + baseline
    
    def augment_data(self, X, y, augmentation_factor=3, drift_strength=0.05):
        """å¯¹æ•°æ®è¿›è¡ŒåŸºçº¿æ¼‚ç§»å¢å¼º"""
        print(f"ğŸ“ˆ æ­£åœ¨è¿›è¡ŒåŸºçº¿æ¼‚ç§»æ•°æ®å¢å¼º...")
        print(f"   å¢å¼ºå€æ•°: {augmentation_factor}")
        print(f"   æ¼‚ç§»å¼ºåº¦: {drift_strength}")
        
        X_augmented_list = [X]
        y_augmented_list = [y]
        
        for i in range(augmentation_factor):
            X_aug = np.array([self.add_baseline_drift(spectrum, drift_strength) for spectrum in X])
            X_augmented_list.append(X_aug)
            y_augmented_list.append(y)
        
        X_augmented = np.vstack(X_augmented_list)
        y_augmented = np.hstack(y_augmented_list)
        
        print(f"   åŸå§‹æ•°æ®é‡: {len(X)} -> å¢å¼ºåæ•°æ®é‡: {len(X_augmented)}")
        
        return X_augmented, y_augmented

class ConsistencyActiveLearning:
    """Consistency-based ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        np.random.seed(random_state)
    
    def create_diverse_models(self, X_train, y_train, n_models=5):
        """
        åˆ›å»ºå¤šä¸ªä¸åŒçš„åŸºç¡€æ¨¡å‹ç”¨äºä¸€è‡´æ€§æ£€æŸ¥
        åŒ…æ‹¬ä¸åŒç±»å‹çš„åˆ†ç±»å™¨ä»¥å¢åŠ å¤šæ ·æ€§
        """
        print(f"ğŸ¯ åˆ›å»º {n_models} ä¸ªå¤šæ ·åŒ–æ¨¡å‹ç”¨äºä¸€è‡´æ€§è¯„ä¼°")
        
        models = []
        model_names = []
        
        try:
            # 1. éšæœºæ£®æ— (ä¸åŒå‚æ•°)
            rf1 = RandomForestClassifier(
                n_estimators=50, 
                max_depth=10, 
                random_state=self.random_state,
                n_jobs=-1
            )
            models.append(rf1)
            model_names.append("RandomForest_1")
            
            rf2 = RandomForestClassifier(
                n_estimators=100, 
                max_depth=15,
                min_samples_split=5,
                random_state=self.random_state + 1,
                n_jobs=-1
            )
            models.append(rf2)
            model_names.append("RandomForest_2")
            
            # 2. SVM (ä¸åŒæ ¸å‡½æ•°)
            if len(X_train) < 1000:  # SVMå¯¹å¤§æ•°æ®é›†è¾ƒæ…¢
                svm_rbf = SVC(
                    kernel='rbf', 
                    probability=True,
                    random_state=self.random_state,
                    C=1.0
                )
                models.append(svm_rbf)
                model_names.append("SVM_RBF")
                
                svm_linear = SVC(
                    kernel='linear', 
                    probability=True,
                    random_state=self.random_state,
                    C=0.1
                )
                models.append(svm_linear)
                model_names.append("SVM_Linear")
            
            # 3. ç¥ç»ç½‘ç»œ (ä¸åŒæ¶æ„)
            mlp = MLPClassifier(
                hidden_layer_sizes=(100, 50),
                random_state=self.random_state,
                max_iter=500,
                alpha=0.01
            )
            models.append(mlp)
            model_names.append("MLP")
            
            # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
            trained_models = []
            successful_names = []
            
            for i, (model, name) in enumerate(zip(models, model_names)):
                try:
                    print(f"   è®­ç»ƒæ¨¡å‹ {i+1}/{len(models)}: {name}")
                    model.fit(X_train, y_train)
                    trained_models.append(model)
                    successful_names.append(name)
                except Exception as e:
                    print(f"   âš ï¸ æ¨¡å‹ {name} è®­ç»ƒå¤±è´¥: {e}")
                    continue
            
            print(f"   æˆåŠŸè®­ç»ƒ {len(trained_models)} ä¸ªæ¨¡å‹")
            return trained_models, successful_names
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå¤šæ ·åŒ–æ¨¡å‹å¤±è´¥: {e}")
            # å›é€€åˆ°å•ä¸€éšæœºæ£®æ—æ¨¡å‹
            rf = RandomForestClassifier(n_estimators=50, random_state=self.random_state, n_jobs=-1)
            rf.fit(X_train, y_train)
            return [rf], ["RandomForest_Fallback"]
    
    def compute_prediction_consistency(self, models, X_pool):
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹å¯¹å€™é€‰æ ·æœ¬é¢„æµ‹çš„ä¸€è‡´æ€§
        ä¸€è‡´æ€§ä½çš„æ ·æœ¬æ›´æœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒä»¬åœ¨ä¸åŒæ¨¡å‹é—´äº§ç”Ÿåˆ†æ­§
        """
        print(f"ğŸ¯ è®¡ç®— {len(models)} ä¸ªæ¨¡å‹çš„é¢„æµ‹ä¸€è‡´æ€§")
        
        try:
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
            all_predictions = []
            
            for i, model in enumerate(models):
                try:
                    if hasattr(model, 'predict_proba'):
                        # ä½¿ç”¨æ¦‚ç‡é¢„æµ‹
                        proba = model.predict_proba(X_pool)
                        pred = np.argmax(proba, axis=1)
                    else:
                        pred = model.predict(X_pool)
                    all_predictions.append(pred)
                except Exception as e:
                    print(f"   âš ï¸ æ¨¡å‹ {i} é¢„æµ‹å¤±è´¥: {e}")
                    continue
            
            if len(all_predictions) == 0:
                print("âš ï¸ æ‰€æœ‰æ¨¡å‹é¢„æµ‹éƒ½å¤±è´¥")
                return np.zeros(len(X_pool))
            
            all_predictions = np.array(all_predictions)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¸€è‡´æ€§åˆ†æ•°
            consistency_scores = []
            
            for i in range(len(X_pool)):
                sample_predictions = all_predictions[:, i]
                
                # è®¡ç®—ä¸€è‡´æ€§ï¼šé¢„æµ‹ç›¸åŒçš„æ¨¡å‹æ•°é‡æ¯”ä¾‹
                unique_preds, counts = np.unique(sample_predictions, return_counts=True)
                max_agreement = np.max(counts)
                consistency = max_agreement / len(all_predictions)
                
                # æˆ‘ä»¬è¦é€‰æ‹©ä¸€è‡´æ€§ä½çš„æ ·æœ¬ï¼ˆåˆ†æ­§å¤§çš„æ ·æœ¬ï¼‰
                inconsistency_score = 1.0 - consistency
                consistency_scores.append(inconsistency_score)
            
            consistency_scores = np.array(consistency_scores)
            
            # ç»Ÿè®¡ä¿¡æ¯
            avg_consistency = 1.0 - np.mean(consistency_scores)
            min_consistency = 1.0 - np.max(consistency_scores)
            max_consistency = 1.0 - np.min(consistency_scores)
            
            print(f"   å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.4f}")
            print(f"   æœ€å°ä¸€è‡´æ€§: {min_consistency:.4f}")
            print(f"   æœ€å¤§ä¸€è‡´æ€§: {max_consistency:.4f}")
            print(f"   ä¸€è‡´æ€§æ ‡å‡†å·®: {np.std(1.0 - consistency_scores):.4f}")
            
            return consistency_scores
            
        except Exception as e:
            print(f"âš ï¸ ä¸€è‡´æ€§è®¡ç®—å¤±è´¥: {e}")
            return np.random.random(len(X_pool))  # å›é€€åˆ°éšæœºåˆ†æ•°
    
    def consistency_sampling(self, models, X_pool, n_samples=50):
        """
        Consistency-based é‡‡æ ·ç­–ç•¥
        é€‰æ‹©å¤šä¸ªæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§æœ€ä½çš„æ ·æœ¬ï¼ˆåˆ†æ­§æœ€å¤§çš„æ ·æœ¬ï¼‰
        """
        print(f"ğŸ¯ æ‰§è¡ŒConsistencyé‡‡æ ·ï¼Œé€‰æ‹© {n_samples} ä¸ªæ ·æœ¬")
        
        try:
            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
            inconsistency_scores = self.compute_prediction_consistency(models, X_pool)
            
            # é€‰æ‹©ä¸ä¸€è‡´æ€§æœ€é«˜çš„æ ·æœ¬ï¼ˆä¸€è‡´æ€§æœ€ä½çš„æ ·æœ¬ï¼‰
            selected_indices = np.argsort(inconsistency_scores)[-n_samples:]
            
            # ç»Ÿè®¡ä¿¡æ¯
            avg_inconsistency = np.mean(inconsistency_scores[selected_indices])
            max_inconsistency = np.max(inconsistency_scores[selected_indices])
            min_inconsistency = np.min(inconsistency_scores[selected_indices])
            
            print(f"   å¹³å‡ä¸ä¸€è‡´æ€§: {avg_inconsistency:.4f}")
            print(f"   æœ€å¤§ä¸ä¸€è‡´æ€§: {max_inconsistency:.4f}")
            print(f"   æœ€å°ä¸ä¸€è‡´æ€§: {min_inconsistency:.4f}")
            print(f"   ä¸ä¸€è‡´æ€§æ ‡å‡†å·®: {np.std(inconsistency_scores[selected_indices]):.4f}")
            
            return selected_indices, inconsistency_scores[selected_indices]
            
        except Exception as e:
            print(f"âš ï¸ Consistencyé‡‡æ ·å¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºé‡‡æ ·
            return np.random.choice(len(X_pool), n_samples, replace=False), np.array([])
    
    def active_learning_selection(self, X_augmented, y_augmented, n_samples=200, 
                                 validation_split=0.2):
        """
        æ‰§è¡ŒConsistency-basedä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©
        """
        print(f"\nğŸ¯ å¼€å§‹Consistency-basedä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©")
        print(f"æ€»æ ·æœ¬æ•°: {len(X_augmented)}")
        print(f"ç›®æ ‡é€‰æ‹©æ•°: {n_samples}")
        
        # 1. åˆå§‹è®­ç»ƒé›†åˆ†å‰²
        n_initial = min(100, len(X_augmented) // 4)  # åˆå§‹è®­ç»ƒé›†å¤§å°
        initial_indices = np.random.choice(len(X_augmented), n_initial, replace=False)
        
        X_initial = X_augmented[initial_indices]
        y_initial = y_augmented[initial_indices]
        
        # å‰©ä½™æ ·æœ¬ä½œä¸ºå€™é€‰æ± 
        remaining_indices = np.array([i for i in range(len(X_augmented)) if i not in initial_indices])
        X_pool = X_augmented[remaining_indices]
        y_pool = y_augmented[remaining_indices]
        
        print(f"åˆå§‹è®­ç»ƒé›†: {len(X_initial)} æ ·æœ¬")
        print(f"å€™é€‰æ± : {len(X_pool)} æ ·æœ¬")
        
        # 2. åˆ›å»ºå¤šæ ·åŒ–æ¨¡å‹é›†åˆ
        models, model_names = self.create_diverse_models(X_initial, y_initial)
        print(f"æˆåŠŸåˆ›å»ºæ¨¡å‹: {model_names}")
        
        # 3. æ‰§è¡ŒConsistencyé‡‡æ ·
        if len(X_pool) <= n_samples - len(X_initial):
            # å¦‚æœå€™é€‰æ± æ ·æœ¬ä¸è¶³ï¼Œå…¨éƒ¨é€‰æ‹©
            selected_pool_indices = np.arange(len(X_pool))
            selected_scores = np.array([])
        else:
            selected_pool_indices, selected_scores = self.consistency_sampling(
                models, X_pool, n_samples - len(X_initial)
            )
        
        # 4. ç»„åˆæœ€ç»ˆé€‰æ‹©çš„æ ·æœ¬
        final_indices = np.concatenate([
            initial_indices,
            remaining_indices[selected_pool_indices]
        ])
        
        X_selected = X_augmented[final_indices]
        y_selected = y_augmented[final_indices]
        
        print(f"\nâœ… Consistencyä¸»åŠ¨å­¦ä¹ é€‰æ‹©å®Œæˆ")
        print(f"æœ€ç»ˆé€‰æ‹©æ ·æœ¬æ•°: {len(X_selected)}")
        print(f"é€‰æ‹©ç‡: {len(X_selected)/len(X_augmented)*100:.1f}%")
        
        return X_selected, y_selected, final_indices, selected_scores

def print_model_architecture(model, model_name="AutoKerasæ¨¡å‹"):
    """æ‰“å°AutoKerasæœç´¢å‡ºæ¥çš„ç½‘ç»œç»“æ„"""
    print(f"\n" + "="*70)
    print(f"           {model_name} - æœç´¢å‡ºçš„ç½‘ç»œæ¶æ„è¯¦æƒ…")
    print("="*70)
    
    try:
        if hasattr(model, 'export_model'):
            best_model = model.export_model()
            print("ğŸ“‹ AutoKerasæœç´¢å‡ºçš„æœ€ä½³ç½‘ç»œæ¶æ„æ‘˜è¦:")
            print("-" * 70)
            best_model.summary()
            print(f"\nğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {best_model.count_params():,}")
            
            layer_types = [layer.__class__.__name__ for layer in best_model.layers]
            has_conv = any('Conv' in layer_type for layer_type in layer_types)
            has_lstm = any('LSTM' in layer_type for layer_type in layer_types)
            has_gru = any('GRU' in layer_type for layer_type in layer_types)
            has_dense = any('Dense' in layer_type for layer_type in layer_types)
            
            architecture_components = []
            if has_conv:
                architecture_components.append("CNN")
            if has_lstm:
                architecture_components.append("LSTM")
            if has_gru:
                architecture_components.append("GRU")
            if has_dense:
                architecture_components.append("Dense")
            
            print(f"ğŸ¯ æ¶æ„ç»„æˆ: {' + '.join(architecture_components)}")
                
    except Exception as e:
        print(f"âŒ æ‰“å°æ¶æ„æ—¶å‡ºé”™: {str(e)}")
    
    print("="*70)

def process_spectrum_data_with_consistency_active_learning(file_path):
    """
    CARSç‰¹å¾é€‰æ‹© + åŸºçº¿æ¼‚ç§»æ•°æ®å¢å¼º + Consistencyä¸»åŠ¨å­¦ä¹  + AutoKeras CNN+RNN
    """
    print("="*80)
    print("   CARS + åŸºçº¿æ¼‚ç§»å¢å¼º + Consistencyä¸»åŠ¨å­¦ä¹  + AutoKeras CNN+RNN")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("æ­¥éª¤1: æ­£åœ¨åŠ è½½æ•°æ®...")
    data = pd.read_csv(file_path)
    data = data.dropna()
    X = data.iloc[:, 1:-1].values
    y = data.iloc[:, -1].values
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")

    # 2. æ ‡ç­¾ç¼–ç 
    print("\næ­¥éª¤2: æ­£åœ¨è¿›è¡Œæ ‡ç­¾ç¼–ç ...")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    print(f"æ ‡ç­¾æ˜ å°„: {dict(zip(le.classes_, range(len(le.classes_))))}")

    # 3. SGé¢„å¤„ç†
    print("\næ­¥éª¤3: æ­£åœ¨è¿›è¡ŒSGé¢„å¤„ç†...")
    X_sg = savgol_filter(X, window_length=5, polyorder=2, axis=1)

    # 4. æ•°æ®åˆ’åˆ†
    print("\næ­¥éª¤4: æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_sg, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
    )

    # 5. æ•°æ®æ ‡å‡†åŒ–
    print("\næ­¥éª¤5: æ­£åœ¨è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 6. CARSç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤6: æ­£åœ¨è¿›è¡ŒCARSç‰¹å¾é€‰æ‹©...")
    cars = CARS(n_iterations=50, cv_folds=5)
    cars.fit(X_train_scaled, y_train)
    
    X_train_cars = cars.transform(X_train_scaled)
    X_test_cars = cars.transform(X_test_scaled)
    print(f"CARSé€‰æ‹©äº† {X_train_cars.shape[1]} ä¸ªç‰¹å¾")
    print(f"ç‰¹å¾é€‰æ‹©ç‡: {X_train_cars.shape[1]}/{X.shape[1]} = {X_train_cars.shape[1]/X.shape[1]*100:.2f}%")

    # 7. åŸºçº¿æ¼‚ç§»æ•°æ®å¢å¼º
    print("\næ­¥éª¤7: æ­£åœ¨è¿›è¡ŒåŸºçº¿æ¼‚ç§»æ•°æ®å¢å¼º...")
    augmenter = BaselineDriftAugmentation(random_state=42)
    X_train_augmented, y_train_augmented = augmenter.augment_data(
        X_train_cars, y_train, 
        augmentation_factor=3,
        drift_strength=0.05
    )

    # 8. Consistencyä¸»åŠ¨å­¦ä¹ é‡‡æ ·
    print("\næ­¥éª¤8: æ­£åœ¨è¿›è¡ŒConsistencyä¸»åŠ¨å­¦ä¹ é‡‡æ ·...")
    active_learner = ConsistencyActiveLearning(random_state=42)
    X_selected, y_selected, selected_indices, scores = active_learner.active_learning_selection(
        X_train_augmented, y_train_augmented,
        n_samples=min(400, len(X_train_augmented) // 2),  # é€‰æ‹©ä¸€åŠæ ·æœ¬
        validation_split=0.2
    )

    # 9. è°ƒæ•´æ•°æ®å½¢çŠ¶ä»¥é€‚åº”CNN+RNN
    print("\næ­¥éª¤9: æ­£åœ¨è°ƒæ•´æ•°æ®å½¢çŠ¶...")
    X_train_reshaped = X_selected.reshape(X_selected.shape[0], 1, X_selected.shape[1])
    X_test_reshaped = X_test_cars.reshape(X_test_cars.shape[0], 1, X_test_cars.shape[1])
    y_train_final = y_selected.astype(np.int32)
    y_test_final = y_test.astype(np.int32)
    
    print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_train_reshaped.shape}")
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {X_test_reshaped.shape}")

    # 10. åˆ›å»ºAutoKerasæ¨¡å‹
    print("\næ­¥éª¤10: æ­£åœ¨åˆ›å»ºAutoKeras CNN+RNNæ¨¡å‹...")
    input_node = ak.Input()
    output_node = ak.Normalization()(input_node)
    output_node = ak.ConvBlock(num_blocks=2, num_layers=2, dropout=0.1)(output_node)
    output_node = ak.RNNBlock(layer_type='lstm', num_layers=1, bidirectional=False)(output_node)
    output_node = ak.ClassificationHead()(output_node)
    
    model = ak.AutoModel(
        inputs=input_node,
        outputs=output_node,
        overwrite=True,
        max_trials=8
    )
    
    print("æ­£åœ¨å¼€å§‹AutoKerasæ¨¡å‹æœç´¢å’Œè®­ç»ƒ...")
    
    # 11. è®­ç»ƒæ¨¡å‹
    model.fit(
        X_train_reshaped, 
        y_train_final,
        validation_split=0.2,
        epochs=800,
        verbose=1
    )
    
    print("AutoKerasæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # æ‰“å°ç½‘ç»œç»“æ„
    print_model_architecture(model, "CARS + åŸºçº¿æ¼‚ç§»å¢å¼º + Consistencyä¸»åŠ¨å­¦ä¹  + AutoKeras CNN+RNN")
    
    # 12. æ¨¡å‹è¯„ä¼°
    print("\næ­¥éª¤11: æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    y_pred = model.predict(X_test_reshaped)
    
    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:
        y_pred_final = np.argmax(y_pred, axis=1)
    else:
        y_pred_final = y_pred.flatten().astype(np.int32)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_test_final, y_pred_final) * 100
    precision = precision_score(y_test_final, y_pred_final, average='weighted', zero_division=0) * 100
    recall = recall_score(y_test_final, y_pred_final, average='weighted', zero_division=0) * 100
    f1 = f1_score(y_test_final, y_pred_final, average='weighted', zero_division=0) * 100

    print("\n" + "="*80)
    print("  CARS + åŸºçº¿æ¼‚ç§»å¢å¼º + Consistencyä¸»åŠ¨å­¦ä¹  + AutoKeras æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*80)
    print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"ç²¾å‡†ç‡: {precision:.2f}%")
    print(f"å¬å›ç‡: {recall:.2f}%")
    print(f"F1å€¼: {f1:.2f}%")
    print("="*80)
    
    # æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test_final, y_pred_final, target_names=[str(label) for label in le.classes_]))
    
    # æ˜¾ç¤ºä¸»åŠ¨å­¦ä¹ æ•ˆæœæ€»ç»“
    print(f"\nğŸ¯ Consistencyä¸»åŠ¨å­¦ä¹ æ•ˆæœæ€»ç»“:")
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"CARSé€‰æ‹©ç‰¹å¾: {X_train_cars.shape[1]} ({X_train_cars.shape[1]/X.shape[1]*100:.1f}%)")
    print(f"å¢å¼ºåæ€»æ ·æœ¬: {len(X_train_augmented)}")
    print(f"ä¸»åŠ¨å­¦ä¹ é€‰æ‹©: {len(X_selected)} ({len(X_selected)/len(X_train_augmented)*100:.1f}%)")
    print(f"æ•°æ®å‹ç¼©ç‡: {len(X_selected)/len(X_train_augmented):.2f}")
    print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½: {accuracy:.2f}%")
    if len(scores) > 0:
        print(f"é€‰æ‹©æ ·æœ¬å¹³å‡ä¸ä¸€è‡´æ€§: {np.mean(scores):.4f}")
    
    # è·å–æœ€ç»ˆæ¨¡å‹
    exported_model = model.export_model()
    
    return {
        'method': 'Consistencyä¸»åŠ¨å­¦ä¹ ',
        'model': exported_model,
        'scaler': scaler,
        'cars': cars,
        'label_encoder': le,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'original_features': X.shape[1],
        'selected_features': X_train_cars.shape[1],
        'augmented_samples': len(X_train_augmented),
        'selected_samples': len(X_selected),
        'selection_ratio': len(X_selected)/len(X_train_augmented),
        'avg_inconsistency': np.mean(scores) if len(scores) > 0 else 0
    }

if __name__ == '__main__':
    # é…ç½®GPU
    gpu_available = configure_gpu()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    tf.random.set_seed(42)
    
    data_path = r'C:\Users\Administrator\Desktop\ç®¡é“æ·¤æ³¥é¡¹ç›®\å…‰è°±\è¿‘çº¢å¤–æ•°æ®\4.1æ•°æ®-è¿‘çº¢å¤–\65â„ƒ-è¿‡ç­›\65çƒ˜å¹²è¿‡ç­›.csv'
    
    print("ğŸ“ˆ å¯åŠ¨CARS + åŸºçº¿æ¼‚ç§»å¢å¼º + Consistencyä¸»åŠ¨å­¦ä¹  + AutoKeras CNN+RNNå®éªŒ...")
    if gpu_available:
        print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
    
    try:
        result = process_spectrum_data_with_consistency_active_learning(data_path)
        
        print(f"\nğŸ‰ Consistencyä¸»åŠ¨å­¦ä¹ å®éªŒå®Œæˆï¼")
        print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½: {result['accuracy']:.2f}%")
        print(f"æ•°æ®é€‰æ‹©ç‡: {result['selection_ratio']:.2f}")
        print(f"æ ·æœ¬å‹ç¼©: {result['augmented_samples']} -> {result['selected_samples']}")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}") 