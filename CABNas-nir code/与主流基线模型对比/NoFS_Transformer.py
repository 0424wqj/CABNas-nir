import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from scipy.signal import savgol_filter
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Embedding
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
warnings.filterwarnings('ignore')

# GPUé…ç½®
def configure_gpu():
    """é…ç½®GPUä½¿ç”¨"""
    physical_devices = tf.config.list_physical_devices('GPU')
    print(f"æ£€æµ‹åˆ° {len(physical_devices)} ä¸ªGPUè®¾å¤‡:")
    for device in physical_devices:
        print(f"  - {device}")
    
    if len(physical_devices) > 0:
        try:
            for device in physical_devices:
                tf.config.experimental.set_memory_growth(device, True)
            print("âœ… GPUé…ç½®æˆåŠŸï¼Œå¯ç”¨å†…å­˜å¢é•¿")
            return True
        except Exception as e:
            print(f"âš ï¸ GPUé…ç½®å¤±è´¥: {e}")
            return False
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False

class TransformerBlock(tf.keras.layers.Layer):
    """Transformerç¼–ç å™¨å—"""
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def create_transformer_model(input_shape, num_classes, embed_dim=128, num_heads=8, ff_dim=512, num_transformer_blocks=2):
    """åˆ›å»ºTransformeråˆ†ç±»æ¨¡å‹"""
    inputs = Input(shape=input_shape)
    
    # å°†1Då…‰è°±æ•°æ®é‡å¡‘ä¸ºåºåˆ—
    x = tf.expand_dims(inputs, axis=-1)  # (batch_size, seq_len, 1)
    
    # ä½ç½®ç¼–ç å’ŒæŠ•å½±
    x = Dense(embed_dim)(x)
    
    # æ·»åŠ Transformerå—
    for _ in range(num_transformer_blocks):
        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)
    
    # å…¨å±€å¹³å‡æ± åŒ–
    x = GlobalAveragePooling1D()(x)
    
    # åˆ†ç±»å¤´
    x = Dropout(0.3)(x)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(0.2)(x)
    outputs = Dense(num_classes, activation="softmax")(x)
    
    model = Model(inputs, outputs)
    return model

def process_spectrum_data(file_path):
    """
    ä¸ä½¿ç”¨ç‰¹å¾é€‰æ‹©çš„Transformeråˆ†ç±»
    """
    print("="*60)
    print("        æ— ç‰¹å¾é€‰æ‹© - Transformeråˆ†ç±»")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    data = pd.read_csv(file_path)
    data = data.dropna()
    X = data.iloc[:, 1:-1].values
    y = data.iloc[:, -1].values
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")

    # 2. æ ‡ç­¾ç¼–ç 
    print("æ­£åœ¨è¿›è¡Œæ ‡ç­¾ç¼–ç ...")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    num_classes = len(le.classes_)
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"æ ‡ç­¾æ˜ å°„: {dict(zip(le.classes_, range(len(le.classes_))))}")

    # 3. SGé¢„å¤„ç†
    print("æ­£åœ¨è¿›è¡ŒSGé¢„å¤„ç†...")
    X_sg = savgol_filter(X, window_length=5, polyorder=2, axis=1)

    # 4. æ•°æ®åˆ’åˆ†
    print("æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_sg, y_encoded, test_size=0.4, random_state=44, stratify=y_encoded
    )

    # 5. æ•°æ®æ ‡å‡†åŒ–
    print("æ­£åœ¨è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"ä½¿ç”¨å…¨éƒ¨ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train_scaled.shape}")
    print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test_scaled.shape}")

    # 6. åˆ›å»ºå’Œç¼–è¯‘Transformeræ¨¡å‹
    print("æ­£åœ¨åˆ›å»ºTransformeræ¨¡å‹...")
    model = create_transformer_model(
        input_shape=(X_train_scaled.shape[1],),
        num_classes=num_classes,
        embed_dim=128,
        num_heads=8,
        ff_dim=512,
        num_transformer_blocks=3
    )
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("æ¨¡å‹ç»“æ„:")
    model.summary()

    # 7. è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # 8. è®­ç»ƒæ¨¡å‹
    print("æ­£åœ¨è¿›è¡ŒTransformeræ¨¡å‹è®­ç»ƒ...")
    history = model.fit(
        X_train_scaled, y_train,
        validation_split=0.2,
        epochs=200,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # 9. æ¨¡å‹è¯„ä¼°
    print("æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    y_pred_proba = model.predict(X_test_scaled)
    y_pred = np.argmax(y_pred_proba, axis=1)

    accuracy = accuracy_score(y_test, y_pred) * 100
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100

    print("\n--- æ¨¡å‹è¯„ä¼°ç»“æœ (Transformer - æ— ç‰¹å¾é€‰æ‹©) ---")
    print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"ç²¾å‡†ç‡: {precision:.2f}%")
    print(f"å¬å›ç‡: {recall:.2f}%")
    print(f"F1å€¼: {f1:.2f}%")
    
    # æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=[str(label) for label in le.classes_]))
    
    # æ˜¾ç¤ºç‰¹å¾ä½¿ç”¨ä¿¡æ¯
    print(f"\nç‰¹å¾ä½¿ç”¨ä¿¡æ¯:")
    print(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]} (å…¨éƒ¨ç‰¹å¾)")
    print(f"ç‰¹å¾é€‰æ‹©æ–¹æ³•: æ— ")
    print(f"æ¨¡å‹ç±»å‹: Transformer")
    print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
    
    # æ˜¾ç¤ºè®­ç»ƒå†å²
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    
    print(f"\nè®­ç»ƒå†å²:")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.4f}")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_features': X_train_scaled.shape[1],
        'method': 'Transformer-æ— ç‰¹å¾é€‰æ‹©',
        'model': model,
        'scaler': scaler,
        'label_encoder': le,
        'history': history,
        'num_classes': num_classes
    }

if __name__ == '__main__':
    # é…ç½®GPU
    gpu_available = configure_gpu()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    tf.random.set_seed(42)
    
    data_path = r'C:\Users\Administrator\Desktop\ç®¡é“æ·¤æ³¥é¡¹ç›®\å…‰è°±\è¿‘çº¢å¤–æ•°æ®\4.1æ•°æ®-è¿‘çº¢å¤–\65â„ƒ-è¿‡ç­›\65çƒ˜å¹²è¿‡ç­›.csv'
    
    print("ğŸ“ˆ å¼€å§‹Transformeræ— ç‰¹å¾é€‰æ‹©å®éªŒ...")
    if gpu_available:
        print("ğŸš€ ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
    
    try:
        results = process_spectrum_data(data_path)
        print(f"\nğŸ‰ Transformerå®éªŒå®Œæˆ!")
        print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½: {results['accuracy']:.2f}%")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}") 