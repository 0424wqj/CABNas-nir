import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from scipy.signal import savgol_filter
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Reshape, Lambda
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import pywt
import warnings
warnings.filterwarnings('ignore')

# GPUé…ç½®
def configure_gpu():
    """é…ç½®GPUä½¿ç”¨"""
    physical_devices = tf.config.list_physical_devices('GPU')
    print(f"æ£€æµ‹åˆ° {len(physical_devices)} ä¸ªGPUè®¾å¤‡:")
    for device in physical_devices:
        print(f"  - {device}")
    
    if len(physical_devices) > 0:
        try:
            for device in physical_devices:
                tf.config.experimental.set_memory_growth(device, True)
            print("âœ… GPUé…ç½®æˆåŠŸï¼Œå¯ç”¨å†…å­˜å¢é•¿")
            return True
        except Exception as e:
            print(f"âš ï¸ GPUé…ç½®å¤±è´¥: {e}")
            return False
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False

class WaveletLayer(tf.keras.layers.Layer):
    """å°æ³¢å˜æ¢å±‚"""
    def __init__(self, wavelet='db4', levels=3, **kwargs):
        super(WaveletLayer, self).__init__(**kwargs)
        self.wavelet = wavelet
        self.levels = levels
        
    def build(self, input_shape):
        super(WaveletLayer, self).build(input_shape)
        
    def call(self, inputs):
        """æ‰§è¡Œå°æ³¢å˜æ¢"""
        def wavelet_transform(x):
            # å°†å¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå°æ³¢å˜æ¢
            batch_size = tf.shape(x)[0]
            
            def single_transform(signal):
                # å¯¹å•ä¸ªä¿¡å·è¿›è¡Œå°æ³¢å˜æ¢
                signal_np = signal.numpy()
                coeffs = pywt.wavedec(signal_np, self.wavelet, level=self.levels)
                
                # æå–è¿‘ä¼¼ç³»æ•°å’Œè¯¦ç»†ç³»æ•°
                features = []
                for i, coeff in enumerate(coeffs):
                    if i == 0:  # è¿‘ä¼¼ç³»æ•°
                        features.extend(coeff.tolist())
                    else:  # è¯¦ç»†ç³»æ•°
                        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
                        features.extend([
                            np.mean(coeff),
                            np.std(coeff),
                            np.var(coeff),
                            np.max(coeff),
                            np.min(coeff)
                        ])
                
                return np.array(features, dtype=np.float32)
            
            # ä½¿ç”¨tf.py_functionæ¥åº”ç”¨å°æ³¢å˜æ¢
            transformed = tf.py_function(
                func=lambda x: tf.stack([single_transform(x[i]) for i in range(tf.shape(x)[0])]),
                inp=[x],
                Tout=tf.float32
            )
            
            return transformed
        
        return wavelet_transform(inputs)
    
    def compute_output_shape(self, input_shape):
        # ä¼°ç®—è¾“å‡ºå½¢çŠ¶ï¼ˆè¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼‰
        estimated_length = input_shape[-1] // (2 ** self.levels) + self.levels * 5
        return (input_shape[0], estimated_length)

def wavelet_features_extraction(data, wavelet='db4', levels=3):
    """ä½¿ç”¨PyWaveletsè¿›è¡Œå°æ³¢ç‰¹å¾æå–"""
    features_list = []
    
    for i, signal in enumerate(data):
        try:
            # æ‰§è¡Œå°æ³¢åˆ†è§£
            coeffs = pywt.wavedec(signal, wavelet, level=levels)
            
            # æå–ç‰¹å¾
            features = []
            
            # è¿‘ä¼¼ç³»æ•° (ä½é¢‘æˆåˆ†)
            approx = coeffs[0]
            features.extend([
                np.mean(approx),
                np.std(approx),
                np.var(approx),
                np.max(approx),
                np.min(approx),
                np.median(approx)
            ])
            
            # è¯¦ç»†ç³»æ•° (é«˜é¢‘æˆåˆ†)
            for j, detail in enumerate(coeffs[1:], 1):
                features.extend([
                    np.mean(detail),
                    np.std(detail),
                    np.var(detail),
                    np.max(detail),
                    np.min(detail),
                    np.median(detail),
                    np.sum(np.abs(detail)),  # èƒ½é‡
                    np.sqrt(np.mean(detail**2))  # RMS
                ])
            
            # æ·»åŠ å°æ³¢åŒ…åˆ†è§£çš„é¢å¤–ç‰¹å¾
            features.extend([
                np.sum([np.sum(c**2) for c in coeffs]),  # æ€»èƒ½é‡
                len([c for c in coeffs[0] if abs(c) > np.std(coeffs[0])]),  # æ˜¾è‘—ç³»æ•°æ•°é‡
            ])
            
            features_list.append(features)
            
        except Exception as e:
            print(f"å¤„ç†ç¬¬{i}ä¸ªä¿¡å·æ—¶å‡ºé”™: {e}")
            # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨é›¶å¡«å……
            features_list.append([0] * 50)  # å‡è®¾ç‰¹å¾é•¿åº¦ä¸º50
        
        if (i + 1) % 100 == 0:
            print(f"å·²å¤„ç† {i + 1}/{len(data)} ä¸ªä¿¡å·")
    
    return np.array(features_list)

def create_wnn_model(input_shape, wavelet_features_shape, num_classes):
    """åˆ›å»ºå°æ³¢ç¥ç»ç½‘ç»œæ¨¡å‹"""
    # åŸå§‹ä¿¡å·è¾“å…¥
    signal_input = Input(shape=input_shape, name='signal_input')
    
    # å°æ³¢ç‰¹å¾è¾“å…¥
    wavelet_input = Input(shape=wavelet_features_shape, name='wavelet_input')
    
    # åŸå§‹ä¿¡å·å¤„ç†åˆ†æ”¯
    signal_branch = Dense(256, activation='relu')(signal_input)
    signal_branch = Dropout(0.3)(signal_branch)
    signal_branch = Dense(128, activation='relu')(signal_branch)
    signal_branch = Dropout(0.2)(signal_branch)
    
    # å°æ³¢ç‰¹å¾å¤„ç†åˆ†æ”¯
    wavelet_branch = Dense(128, activation='relu')(wavelet_input)
    wavelet_branch = Dropout(0.3)(wavelet_branch)
    wavelet_branch = Dense(64, activation='relu')(wavelet_branch)
    wavelet_branch = Dropout(0.2)(wavelet_branch)
    
    # ç‰¹å¾èåˆ
    merged = Concatenate()([signal_branch, wavelet_branch])
    
    # åˆ†ç±»å™¨
    x = Dense(256, activation='relu')(merged)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    outputs = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=[signal_input, wavelet_input], outputs=outputs)
    return model

def process_spectrum_data(file_path):
    """
    ä¸ä½¿ç”¨ç‰¹å¾é€‰æ‹©çš„å°æ³¢ç¥ç»ç½‘ç»œåˆ†ç±»
    """
    print("="*60)
    print("      æ— ç‰¹å¾é€‰æ‹© - å°æ³¢ç¥ç»ç½‘ç»œ(WNN)åˆ†ç±»")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    data = pd.read_csv(file_path)
    data = data.dropna()
    X = data.iloc[:, 1:-1].values
    y = data.iloc[:, -1].values
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")

    # 2. æ ‡ç­¾ç¼–ç 
    print("æ­£åœ¨è¿›è¡Œæ ‡ç­¾ç¼–ç ...")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    num_classes = len(le.classes_)
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"æ ‡ç­¾æ˜ å°„: {dict(zip(le.classes_, range(len(le.classes_))))}")

    # 3. SGé¢„å¤„ç†
    print("æ­£åœ¨è¿›è¡ŒSGé¢„å¤„ç†...")
    X_sg = savgol_filter(X, window_length=5, polyorder=2, axis=1)

    # 4. æ•°æ®åˆ’åˆ†
    print("æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_sg, y_encoded, test_size=0.3, random_state=45, stratify=y_encoded
    )

    # 5. æ•°æ®æ ‡å‡†åŒ–
    print("æ­£åœ¨è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"ä½¿ç”¨å…¨éƒ¨ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train_scaled.shape}")
    print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test_scaled.shape}")

    # 6. å°æ³¢ç‰¹å¾æå–
    print("æ­£åœ¨è¿›è¡Œå°æ³¢ç‰¹å¾æå–...")
    print("æ­£åœ¨æå–è®­ç»ƒé›†å°æ³¢ç‰¹å¾...")
    X_train_wavelet = wavelet_features_extraction(X_train_scaled, wavelet='db4', levels=3)
    print("æ­£åœ¨æå–æµ‹è¯•é›†å°æ³¢ç‰¹å¾...")
    X_test_wavelet = wavelet_features_extraction(X_test_scaled, wavelet='db4', levels=3)
    
    print(f"å°æ³¢ç‰¹å¾å½¢çŠ¶: {X_train_wavelet.shape}")
    
    # æ ‡å‡†åŒ–å°æ³¢ç‰¹å¾
    wavelet_scaler = StandardScaler()
    X_train_wavelet_scaled = wavelet_scaler.fit_transform(X_train_wavelet)
    X_test_wavelet_scaled = wavelet_scaler.transform(X_test_wavelet)

    # 7. åˆ›å»ºå’Œç¼–è¯‘WNNæ¨¡å‹
    print("æ­£åœ¨åˆ›å»ºå°æ³¢ç¥ç»ç½‘ç»œæ¨¡å‹...")
    model = create_wnn_model(
        input_shape=(X_train_scaled.shape[1],),
        wavelet_features_shape=(X_train_wavelet_scaled.shape[1],),
        num_classes=num_classes
    )
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("æ¨¡å‹ç»“æ„:")
    model.summary()

    # 8. è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # 9. è®­ç»ƒæ¨¡å‹
    print("æ­£åœ¨è¿›è¡Œå°æ³¢ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒ...")
    history = model.fit(
        [X_train_scaled, X_train_wavelet_scaled], 
        y_train,
        validation_split=0.2,
        epochs=200,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # 10. æ¨¡å‹è¯„ä¼°
    print("æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    y_pred_proba = model.predict([X_test_scaled, X_test_wavelet_scaled])
    y_pred = np.argmax(y_pred_proba, axis=1)

    accuracy = accuracy_score(y_test, y_pred) * 100
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100

    print("\n--- æ¨¡å‹è¯„ä¼°ç»“æœ (WNN - æ— ç‰¹å¾é€‰æ‹©) ---")
    print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"ç²¾å‡†ç‡: {precision:.2f}%")
    print(f"å¬å›ç‡: {recall:.2f}%")
    print(f"F1å€¼: {f1:.2f}%")
    
    # æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=[str(label) for label in le.classes_]))
    
    # æ˜¾ç¤ºç‰¹å¾ä½¿ç”¨ä¿¡æ¯
    print(f"\nç‰¹å¾ä½¿ç”¨ä¿¡æ¯:")
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]} (å…¨éƒ¨ç‰¹å¾)")
    print(f"å°æ³¢ç‰¹å¾æ•°é‡: {X_train_wavelet_scaled.shape[1]}")
    print(f"æ€»ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1] + X_train_wavelet_scaled.shape[1]}")
    print(f"ç‰¹å¾é€‰æ‹©æ–¹æ³•: æ— ")
    print(f"æ¨¡å‹ç±»å‹: å°æ³¢ç¥ç»ç½‘ç»œ(WNN)")
    print(f"ä½¿ç”¨å°æ³¢: db4")
    print(f"åˆ†è§£å±‚æ•°: 3")
    print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
    
    # æ˜¾ç¤ºè®­ç»ƒå†å²
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    
    print(f"\nè®­ç»ƒå†å²:")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.4f}")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_features': X_train_scaled.shape[1],
        'n_wavelet_features': X_train_wavelet_scaled.shape[1],
        'total_features': X_train_scaled.shape[1] + X_train_wavelet_scaled.shape[1],
        'method': 'WNN-æ— ç‰¹å¾é€‰æ‹©',
        'model': model,
        'scaler': scaler,
        'wavelet_scaler': wavelet_scaler,
        'label_encoder': le,
        'history': history,
        'num_classes': num_classes
    }

if __name__ == '__main__':
    # é…ç½®GPU
    gpu_available = configure_gpu()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    tf.random.set_seed(42)
    
    data_path = r'C:\Users\Administrator\Desktop\ç®¡é“æ·¤æ³¥é¡¹ç›®\å…‰è°±\è¿‘çº¢å¤–æ•°æ®\4.1æ•°æ®-è¿‘çº¢å¤–\65â„ƒ-è¿‡ç­›\65çƒ˜å¹²è¿‡ç­›.csv'
    
    print("ğŸ“ˆ å¼€å§‹å°æ³¢ç¥ç»ç½‘ç»œæ— ç‰¹å¾é€‰æ‹©å®éªŒ...")
    if gpu_available:
        print("ğŸš€ ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
    
    try:
        results = process_spectrum_data(data_path)
        print(f"\nğŸ‰ å°æ³¢ç¥ç»ç½‘ç»œå®éªŒå®Œæˆ!")
        print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½: {results['accuracy']:.2f}%")
        print(f"åŸå§‹ç‰¹å¾: {results['n_features']}")
        print(f"å°æ³¢ç‰¹å¾: {results['n_wavelet_features']}")
        print(f"æ€»ç‰¹å¾æ•°: {results['total_features']}")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}") 